{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18b8036",
   "metadata": {},
   "source": [
    "# Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7dc906",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### VAEs, GANs, and Diffusion Models\n",
    "\n",
    "- **Variational Autoencoders (VAEs)**: Encode data into a latent space and decode it back, allowing for generation of new data.\n",
    "\n",
    "- **Generative Adversarial Networks (GANs)**: Consist of a generator and a discriminator, where the generator creates data and the discriminator evaluates it, leading to improved generation over time.\n",
    "\n",
    "- **Diffusion Models**: Start with noise and iteratively refine it to generate data, often producing high-quality outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89929904",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Self-Attention**: Mechanism that allows the model to weigh the importance of different words in a sequence, enabling it to capture long-range dependencies.\n",
    "\n",
    "- **Positional Encoding**: Adds information about the position of words in a sequence, since transformers do not inherently understand order.\n",
    "\n",
    "- **Multi-Head Attention**: Uses multiple attention mechanisms in parallel, allowing the model to focus on different parts of the input simultaneously.\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **Encoder-Decoder Structure**: The transformer consists of an encoder that processes the input and a decoder that generates the output. Each consists of multiple layers of self-attention and feed-forward networks.\n",
    "- **Layer Normalization**: Applied to stabilize and speed up training by normalizing the inputs to each layer.\n",
    "- **Feed-Forward Networks**: Each layer includes a fully connected feed-forward network that processes the output of the self-attention mechanism.\n",
    "\n",
    "### Training\n",
    "- **Masked Language Modeling**: A training objective where some words in the input are masked, and the model learns to predict them based on the context provided by the unmasked words.\n",
    "- **Next Sentence Prediction**: A task where the model predicts whether a given sentence follows another, helping it learn relationships between sentences.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Natural Language Processing (NLP)**: Transformers are widely used in tasks such as translation, summarization, and question answering.\n",
    "- **Computer Vision**: Transformers have been adapted for image processing tasks, such as object detection and segmentation.\n",
    "- **Reinforcement Learning**: Transformers can be used to model policies and value functions in reinforcement learning settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f829e08",
   "metadata": {},
   "source": [
    "\n",
    "### Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "Another type of generative model that has gained popularity in recent years is the Generative Pre-trained Transformer (GPT). GPT is a type of language model that generates text by predicting the next word in a sequence of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddf8a6",
   "metadata": {},
   "source": [
    "\n",
    "The key idea behind GPT is to learn a transformer model that predicts the next word in a sequence of words. The model is trained using a large corpus of text data and is fine-tuned on a specific task, such as text generation or text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e0481",
   "metadata": {},
   "source": [
    "\n",
    "It learns an underlying distribution of the data and generates new samples by sampling from the learned distribution. So, although its output can be very convincing, it's really just outputting what it thinks is most likely to come next given the input it's been given - it's not actually understanding the text in the way a human would.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4557e",
   "metadata": {},
   "source": [
    "\n",
    "Nevertheless, GPT has been shown to generate high-quality text and has been applied to a wide range of applications, such as text generation, text summarization, and text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea4731",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Pre-training and Fine-tuning**: LLMs are typically pre-trained on large datasets to learn general language patterns and then fine-tuned on specific tasks to improve performance.\n",
    "- **Transfer Learning**: LLMs leverage knowledge learned from one task to improve performance on another, often requiring less data for fine-tuning.\n",
    "- **Zero-shot, One-shot, and Few-shot Learning**: LLMs can perform tasks with little to no task-specific training data, relying on their pre-trained knowledge to generalize to new tasks.\n",
    "- **Contextual Understanding**: LLMs can understand and generate text based on the context provided, allowing them to produce coherent and contextually relevant responses.\n",
    "- **Scalability**: LLMs can be scaled up by increasing the number of parameters, leading to improved performance on various tasks, but also requiring more computational resources.\n",
    "\n",
    "## Applications\n",
    "- **Text Generation**: LLMs can generate coherent and contextually relevant text, making them suitable for applications like chatbots, content creation, and storytelling.\n",
    "- **Question Answering**: LLMs can answer questions based on provided context or general knowledge, making them useful for customer support and information retrieval.\n",
    "- **Translation**: LLMs can translate text between languages, leveraging their understanding of language structure and semantics.\n",
    "- **Summarization**: LLMs can condense long texts into shorter summaries, making them useful for news aggregation and content curation.\n",
    "- **Sentiment Analysis**: LLMs can analyze text to determine sentiment, helping businesses understand customer feedback and opinions.\n",
    "- **Code Generation**: LLMs can generate code snippets based on natural language descriptions, assisting developers in writing software.\n",
    "\n",
    "## Challenges and Limitations\n",
    "- **Bias and Fairness**: LLMs can inherit biases present in the training data, leading to biased outputs. Addressing bias and ensuring fairness is a significant challenge.\n",
    "- **Interpretability**: LLMs are often seen as \"black boxes,\" making it difficult to understand how they arrive at specific outputs. Improving interpretability is an ongoing area of research.\n",
    "- **Resource Intensive**: Training and deploying LLMs require significant computational resources, making them less accessible for smaller organizations.\n",
    "- **Ethical Concerns**: The potential for misuse of LLMs, such as generating misleading or harmful content, raises ethical concerns that need to be addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b54cc6",
   "metadata": {},
   "source": [
    "## Example usage of LLMs in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b54215",
   "metadata": {},
   "source": [
    "Let's deploy a simple LLM using the Hugging Face Transformers library. This example will demonstrate how to use a pre-trained model for text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42028ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/miniconda3/envs/sio209_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-28 17:22:48.268098: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2025-05-28 17:22:48.268127: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-05-28 17:22:48.268132: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2025-05-28 17:22:48.268167: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-28 17:22:48.268178: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Load a pre-trained text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50967ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a galaxy far, far away, I became a powerful hero, and in doing so made more lives possible than I'd ever imagine.\n",
      "\n",
      "I know for a fact that other people didn't realise I had been a hero\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a prompt\n",
    "prompt = \"Once upon a time in a galaxy far, far away\"\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f07e2",
   "metadata": {},
   "source": [
    "This code snippet uses the Hugging Face Transformers library to load a pre-trained GPT-2 model and generate text based on a given prompt. The `pipeline` function simplifies the process of using pre-trained models for various tasks, including text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010cb626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 159783.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.models.cache import make_prompt_cache\n",
    "\n",
    "# Specify the checkpoint\n",
    "checkpoint = \"mlx-community/Qwen2.5-32B-Instruct-4bit\"\n",
    "\n",
    "# Load the corresponding model and tokenizer\n",
    "model, tokenizer = load(path_or_hf_repo=checkpoint)\n",
    "\n",
    "# An example tool, make sure to include a docstring and type hints\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"\n",
    "    A function that multiplies two numbers\n",
    "\n",
    "    Args:\n",
    "        a: The first number to multiply\n",
    "        b: The second number to multiply\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = {\"multiply\": multiply}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea02ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<tool_call>\n",
      "{\"name\": \"multiply\", \"arguments\": {\"a\": 12234585, \"b\": 48838483920}}\n",
      "</tool_call>\n",
      "==========\n",
      "Prompt: 221 tokens, 113.483 tokens-per-sec\n",
      "Generation: 42 tokens, 18.302 tokens-per-sec\n",
      "Peak memory: 18.803 GB\n",
      "<tool_call>\n",
      "{\"name\": \"multiply\", \"arguments\": {\"a\": 12234585, \"b\": 48838483920}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the prompt and conversation history\n",
    "prompt = \"Multiply 12234585 and 48838483920.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tools=list(tools.values())\n",
    ")\n",
    "\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Generate the initial tool call:\n",
    "response = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_tokens=2048,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a5d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The product of 12234585 and 48838483920 is 597518582790373200.\n",
      "==========\n",
      "Prompt: 56 tokens, 78.884 tokens-per-sec\n",
      "Generation: 47 tokens, 13.570 tokens-per-sec\n",
      "Peak memory: 18.803 GB\n"
     ]
    }
   ],
   "source": [
    "# Parse the tool call:\n",
    "# (Note, the tool call format is model specific)\n",
    "tool_open = \"<tool_call>\"\n",
    "tool_close = \"</tool_call>\"\n",
    "start_tool = response.find(tool_open) + len(tool_open)\n",
    "end_tool = response.find(tool_close)\n",
    "tool_call = json.loads(response[start_tool:end_tool].strip())\n",
    "tool_result = tools[tool_call[\"name\"]](**tool_call[\"arguments\"])\n",
    "\n",
    "# Put the tool result in the prompt\n",
    "messages = [{\"role\": \"tool\", \"name\": tool_call[\"name\"], \"content\": tool_result}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Generate the final response:\n",
    "response = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_tokens=2048,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de69e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.multiply(a: float, b: float)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tools.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114dff8",
   "metadata": {},
   "source": [
    "\n",
    "## Multimodal Models\n",
    "- **Definition**: Multimodal models are designed to process and generate data from multiple modalities, such as text, images, and audio. They can understand and generate content that combines different types of information.\n",
    "\n",
    "- **Tokenization**: Multimodal models often use specialized tokenization techniques to handle different types of data. For example, images may be tokenized into patches, while text is tokenized into words or subwords.\n",
    "\n",
    "- **Cross-Modal Attention**: These models utilize attention mechanisms that allow them to focus on relevant parts of different modalities simultaneously, enabling them to learn relationships between text and images or other modalities.\n",
    "\n",
    "\n",
    "- **Applications**: Multimodal models can be used for tasks like image captioning, video analysis, and cross-modal retrieval, where understanding the relationship between different modalities is crucial.\n",
    "\n",
    "- **Examples**: Models like CLIP (Contrastive Language-Image Pre-training) and DALL-E combine text and image understanding, enabling them to generate images from textual descriptions or vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4b1c6",
   "metadata": {},
   "source": [
    "## What about scientific data?\n",
    "\n",
    "- **Scientific Data**: LLMs can be adapted to handle scientific data, such as research papers, experimental results, and datasets. They can assist in literature review, hypothesis generation, and data analysis.\n",
    "- **Applications in Science**: LLMs can be used for tasks like automated literature review, data extraction from scientific texts, and generating hypotheses based on existing research.\n",
    "\n",
    "But, they are not yet able to reliably work with scientific data, and there are many challenges to overcome before they can be used effectively in this domain.\n",
    "\n",
    "In collaboration with experts in Computer Science we are currently working on a project to adapt LLMs to scientific data, focusing on improving their ability to understand and generate scientific content. One of the main challenges is usefully encoding the scientific data, which often involves complex structures and relationships that are not easily captured by standard text-based models.\n",
    "\n",
    "## Conclusion\n",
    "Large Language Models (LLMs) represent a significant advancement in the field of artificial intelligence, enabling machines to understand and generate human-like text. Their applications span various domains, from natural language processing to computer vision and beyond. However, challenges such as bias, interpretability, and resource requirements remain critical areas for ongoing research and development.\n",
    "\n",
    "As LLMs continue to evolve, their potential to transform industries and improve human-computer interaction is immense. The future of LLMs lies in addressing these challenges while expanding their capabilities to handle more complex and diverse data types, including scientific data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fe610",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "Through this course, we have covered a wide range of topics in deep learning for geo/environmental sciences, from the basics of neural networks to advanced topics like self-supervised learning and generative models. I hope you have gained a good understanding of these topics and how they can be applied to geospatial and environmental data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83528ea",
   "metadata": {},
   "source": [
    "\n",
    "We only scratched the surface of deep learning in this course, and there is much more to learn and explore. I encourage you to continue learning and experimenting with deep learning techniques and apply them to your own research or projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11ba80",
   "metadata": {},
   "source": [
    "\n",
    "If you have any questions or need further clarification on any of the topics covered in this course, please feel free to reach out to me. I'm always happy to help and discuss deep learning with you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e9303",
   "metadata": {},
   "source": [
    "### Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0b8e3",
   "metadata": {},
   "source": [
    "\n",
    "Please fill out the course evaluation form to provide feedback on the course and help me improve it for future iterations. Your feedback is valuable and will help me make this course better for future students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c731915",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sio209_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
